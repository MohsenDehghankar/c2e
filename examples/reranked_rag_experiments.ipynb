{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a56dd58",
   "metadata": {},
   "source": [
    "# Reranked RAG Experiments\n",
    "\n",
    "This notebook runs experiments using the top 3 reranked abstracts from PubMed to answer medical causal claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from helpers import llm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00a52a",
   "metadata": {},
   "source": [
    "## Load Reranked RAG Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_df = pd.read_csv(\"reports/reranked_rag_documents.csv\")\n",
    "print(f\"Loaded {len(reranked_df)} claims with reranked top 3 documents\")\n",
    "reranked_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849d5bc",
   "metadata": {},
   "source": [
    "## Setup Ollama Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b57b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = 11434\n",
    "\n",
    "client = llm.setup_ollama_client(host=host, port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e90329",
   "metadata": {},
   "source": [
    "## Define RAG Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31012999",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"\n",
    "You are a biomedical expert specializing in causal inference. \n",
    "\n",
    "Evaluate the following medical causal claim based ONLY on the provided scientific abstracts.\n",
    "\n",
    "ABSTRACTS:\n",
    "{documents}\n",
    "\n",
    "CLAIM: \"{claim}\"\n",
    "\n",
    "Carefully analyze the evidence in the abstracts. If the abstracts support the claim, respond with SUPPORTED. If they contradict the claim, respond with CONTRADICT.\n",
    "\n",
    "Provide your reasoning, cite relevant papers by PMID, and then give your final answer.\n",
    "\n",
    "Final Answer: [SUPPORTED or CONTRADICT]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0779ca50",
   "metadata": {},
   "source": [
    "## Run Reranked RAG Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"reports/reranked_rag_results.csv\"\n",
    "\n",
    "# Check if output file exists to determine if we need to write headers\n",
    "file_exists = os.path.isfile(output_file)\n",
    "\n",
    "models = [\n",
    "    \"deepseek-r1:32b\",\n",
    "    \"mistral:7b\",\n",
    "    \"llama3.1:8b\",\n",
    "    \"qwen3:30b\",\n",
    "    \"qwen3:8b\",\n",
    "    \"llama3.1:70b\",\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    result = {\n",
    "        \"model\": [],\n",
    "        \"method\": [],\n",
    "        \"claim\": [],\n",
    "        \"keywords\": [],\n",
    "        \"top3_paper_ids\": [],\n",
    "        \"num_selected\": [],\n",
    "        \"top3_abstracts\": [],\n",
    "        \"answer\": [],\n",
    "    }\n",
    "\n",
    "    for idx, row in tqdm(reranked_df.iterrows(), total=len(reranked_df), desc=f\"Reranked RAG - {model}\"):\n",
    "        claim = row[\"claim\"]\n",
    "        top3_abstracts = row[\"top3_abstracts\"]\n",
    "        keywords = row[\"keywords\"]\n",
    "        top3_paper_ids = row[\"top3_paper_ids\"]\n",
    "        num_selected = row[\"num_selected\"]\n",
    "\n",
    "        # Call LLM with RAG prompt using top 3 reranked abstracts\n",
    "        response = llm.call_ollama(\n",
    "            model=model,\n",
    "            prompt=rag_prompt.format(claim=claim, documents=top3_abstracts),\n",
    "            client=client,\n",
    "        )\n",
    "        output = response.get(\"response\", \"NAN\")\n",
    "\n",
    "        result[\"model\"].append(model)\n",
    "        result[\"method\"].append(\"reranked_rag\")\n",
    "        result[\"claim\"].append(claim)\n",
    "        result[\"keywords\"].append(keywords)\n",
    "        result[\"top3_paper_ids\"].append(top3_paper_ids)\n",
    "        result[\"num_selected\"].append(num_selected)\n",
    "        result[\"top3_abstracts\"].append(top3_abstracts)\n",
    "        result[\"answer\"].append(output)\n",
    "\n",
    "    # Convert to DataFrame and append to CSV\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df.to_csv(\n",
    "        output_file,\n",
    "        mode=\"a\" if file_exists else \"w\",\n",
    "        header=not file_exists,\n",
    "        index=False,\n",
    "    )\n",
    "    file_exists = True  # After first write, file exists\n",
    "\n",
    "    print(f\"Completed {model}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"\\nAll experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423f101",
   "metadata": {},
   "source": [
    "## Load and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display final results\n",
    "final_results = pd.read_csv(output_file)\n",
    "print(f\"Total results saved: {len(final_results)}\")\n",
    "final_results.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202d71d",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results by model:\")\n",
    "print(final_results.groupby(\"model\").size())\n",
    "print(\"\\nAverage number of papers used:\")\n",
    "print(final_results.groupby(\"model\")[\"num_selected\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020263c0",
   "metadata": {},
   "source": [
    "## Example: View One Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first result\n",
    "example_idx = 0\n",
    "print(f\"Model: {final_results.iloc[example_idx]['model']}\")\n",
    "print(f\"\\nClaim: {final_results.iloc[example_idx]['claim']}\")\n",
    "print(f\"\\nKeywords: {final_results.iloc[example_idx]['keywords']}\")\n",
    "print(f\"\\nNumber of selected papers: {final_results.iloc[example_idx]['num_selected']}\")\n",
    "print(f\"\\nTop 3 Paper IDs: {final_results.iloc[example_idx]['top3_paper_ids']}\")\n",
    "print(f\"\\nAnswer:\\n{final_results.iloc[example_idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647bf53",
   "metadata": {},
   "source": [
    "## Compare with Original RAG Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fff9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original RAG results for comparison\n",
    "original_rag_results = pd.read_csv(\"reports/rag_results.csv\")\n",
    "\n",
    "print(\"Original RAG - Average number of papers used:\")\n",
    "print(original_rag_results.groupby(\"model\")[\"num_papers\"].mean())\n",
    "print(\"\\nReranked RAG - Average number of papers used:\")\n",
    "print(final_results.groupby(\"model\")[\"num_selected\"].mean())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
